{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Introduction\"></a>\n",
    "# Introduction\n",
    "\n",
    "This notebook's motivation is to create a ready-to-made all-in-one-place of NLP preprocessing and feature extraction techniques and codes. Furthermore, as we would not use all of those techniques simultaneously as it would depend on different specific NLP problems, each task was design as a separate module with its quick and straightforward explanation, then the implementation that we could pick out, plug-and-play independently and conveniently. We will mainly use the [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset for illustration and other dataset such as [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification), and [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) for some specific tasks too.\n",
    "\n",
    "The NLP pipeline could be represent and below image and this notebook only focus on three stages: Text Cleaning, Pre-Processing and Feature Engineering/ Extraction.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1750/1*rJQVqDjbhI3k22lHqa4dFw.png\" align=\"center\"/>\n",
    "\n",
    "The paper [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067) has been inspiring me a lot for this notebook and most of the definition of each tasks also could be found from the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Read and explore data](#Read_and_explore_data)\n",
    "    - [Importing Main Packages](#Importing_Main_Packages)\n",
    "    - [Read the Data](#Read_the_Data)\n",
    "* [Text Cleaning](#Text_Cleaning)\n",
    "    - [Capitalization/ Lower case](#Capitalization)\n",
    "    - [Expand the Contractions](#Expand_the_Contractions)\n",
    "    - [Noise Removal](#Noise_Removal)\n",
    "        - [Remove URLs](#Remove_urls)\n",
    "        - [Remove HTML tags](#Remove_HTML_tags)\n",
    "        - [Remove Non-ASCII](#Remove_Non_ASCII)\n",
    "        - [Remove special characters](#Remove_special_characters)\n",
    "    - [Remove punctuations](#Remove_punctuations)\n",
    "    - [Other Manual Text Cleaning Tasks](#Other_Manual_Text_Cleaning_Tasks)\n",
    "        - [Replace the Typos, slang, acronyms or informal abbreviations](#Replace_Typos)\n",
    "        - [Spelling correction](#Spelling_correction)    \n",
    "* [Text Preprocessing](#Text_Preprocessing)\n",
    "    - [Tokenization](#Tokenization)\n",
    "    - [Remove Stop Words (or/and Frequent words/ Rare words)](#Remove_Stop_Words)\n",
    "    - [Stemming](#Stemming)\n",
    "        - [PorterStemmer](#PorterStemmer)\n",
    "        - [SnowballStemmer](#SnowballStemmer)\n",
    "        - [LancasterStemmer](#LancasterStemmer)\n",
    "    - [Part of Speech Tagging (POS Tagging)](#POS_Tagging)    \n",
    "    - [Lemmatization](#Lemmatization)\n",
    "        - [Lemmatization without POS Tagging](#Lemmatization_wo_pos)\n",
    "        - [Lemmatization with POS tagging](#Lemmatization_w_pos)\n",
    "    - [Other (Optional) Text Preprocessing Techniques:](#Other_Text_Preprocessing)\n",
    "        - [Language Detection](#Language_Detection)\n",
    "* [Text Features Extraction](#Text_Features_Extraction)\n",
    "    - [Weighted Words - Bag of Words (BoW)](#BoW)\n",
    "        - [Frequency Vectors - CountVectorizer](#CountVectorizer)\n",
    "        - [Term Frequency-Inverse Document Frequency](#TF_IDF)\n",
    "    - [Word Embedding](#Word_Embedding)\n",
    "        - [Basic Word Embedding Methods](#Basic_Word_Embedding)\n",
    "            - [Word2Vec](#Word2Vec)\n",
    "            - [Global Vectors for Word Representation](#GloVe)\n",
    "            - [FastText](#FastText)\n",
    "        - [Advanced Word Embedding Methods - Deep Contextualized Word Representations](#Advanced_methods)\n",
    "            - [Bidirectional Encoder Representations from Transformers (BERT)](#BERT)\n",
    "    - [Comparison of Feature Extraction Techniques](#Comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Read_and_explore_data\"></a>\n",
    "\n",
    "# Read and explore data\n",
    "\n",
    "<a id=\"Importing_Main_Packages\"></a>\n",
    "## Importing Main Packages\n",
    "\n",
    "[Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "Version info.: sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\n",
      "pandas version: 2.1.1\n",
      "numpy version: 1.26.0\n",
      "skearn version: 1.3.2\n",
      "re version: 2.2.1\n",
      "nltk version: 3.8.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Version info.:\", sys.version_info)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"skearn version:\", sklearn.__version__)\n",
    "print(\"re version:\", re.__version__)\n",
    "print(\"nltk version:\", nltk.__version__)\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Read_the_Data\"></a>\n",
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv(r\"D:\\Sharif University of Tech\\Data\\NLP Feature Extraction\\Data\\nlp-getting-started\\train.csv\")\n",
    "display(train_df.shape, train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "\n",
       "                                                 text  target  \n",
       "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "32  We always try to bring the heavy. #metal #RT h...       0  \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "34                 Crying out for more! Set me ablaze       0  \n",
       "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I love fruits'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df[~train_df[\"location\"].isnull()].head())\n",
    "display(train_df[train_df[\"target\"] == 0][\"text\"].values[1])\n",
    "display(train_df[train_df[\"target\"] == 1][\"text\"].values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Text_Cleaning\"></a>\n",
    "\n",
    "# Text Cleaning:\n",
    "\n",
    "<a id=\"Capitalization\"></a>\n",
    "## Capitalization/ Lower case\n",
    "The most common approach in text cleaning is capitalization or lower case due to the diversity of capitalization to form a sentence. This technique will project all words in text and document into the same feature space. However, it would also cause problems with exceptional cases such as the USA or UK, which could be solved by replacing typos, slang, acronyms or informal abbreviations technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \n",
       "0       1  our deeds are the reason of this #earthquake m...  \n",
       "1       1             forest fire near la ronge sask. canada  \n",
       "2       1  all residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  just got sent this photo from ruby #alaska as ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text\"].apply(lambda x: x.lower())\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Expand_the_Contractions\"></a>\n",
    "## Expand the Contractions\n",
    "We use the [contractions package](https://github.com/kootenpv/contractions) to expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have.\n",
    "\n",
    "[Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  \n",
      "            You all cannot expand contractions I would think. I would like to know how I would done that! \n",
      "            We are going to the zoo and I do not think I will be home for dinner.\n",
      "            They Are going to the zoo and she will be home for dinner.\n",
      "            We should have do it in here but we should not have eat it\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "test_text = \"\"\"\n",
    "            Y'all can't expand contractions I'd think. I'd like to know how I'd done that! \n",
    "            We're going to the zoo and I don't think I'll be home for dinner.\n",
    "            Theyre going to the zoo and she'll be home for dinner.\n",
    "            We should've do it in here but we shouldn't've eat it\n",
    "            \"\"\"\n",
    "print(\"Test: \", contractions.fix(test_text))\n",
    "\n",
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I can't have kids cuz I got in a bicycle accident &amp; split my testicles. it's impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n",
      "'i cannot have kids cuz i got in a bicycle accident &amp; split my testicles. it is impossible for me to have kids' michael you are the father\n",
      "#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count \n",
      "#raining #flooding #florida #tampabay #tampa 18 or 19 days. i have lost count \n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][67])\n",
    "print(train_df[\"text_clean\"][67])\n",
    "print(train_df[\"text\"][12])\n",
    "print(train_df[\"text_clean\"][12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Noise_Removal\"></a>\n",
    "\n",
    "## Noise Removal \n",
    "Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters). \n",
    "\n",
    "<a id=\"Remove_urls\"></a>\n",
    "### Remove URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "@bbcmtd wholesale markets ablaze \n",
      "INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA\n",
      "inec office in abia set ablaze - \n",
      "Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n",
      "rene ablaze &amp; jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][31])\n",
    "print(train_df[\"text_clean\"][31])\n",
    "print(train_df[\"text\"][37])\n",
    "print(train_df[\"text_clean\"][37])\n",
    "print(train_df[\"text\"][62])\n",
    "print(train_df[\"text_clean\"][62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n",
      "rene ablaze  jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n",
      "NW Michigan #WindStorm (Sheer) Recovery Updates: Leelanau &amp; Grand Traverse - State of Emergency 2b extended http://t.co/OSKfyj8CK7 #BeSafe\n",
      "nw michigan #windstorm (sheer) recovery updates: leelanau  grand traverse - state of emergency 2b extended  #besafe\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][62])\n",
    "print(train_df[\"text_clean\"][62])\n",
    "print(train_df[\"text\"][7385])\n",
    "print(train_df[\"text_clean\"][7385])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-ASCI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "\n",
    "\n",
    "# we could also just do : ''.join([x for x in text if x in string.printable]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J\n",
      "barbados #bridgetown jamaica  two cars set ablaze: santa cruz  head of the st elizabeth police superintende...  \n",
      "#Sismo DETECTADO #JapÌ_n 15:41:07 Seismic intensity 0 Iwate Miyagi JST #?? http://t.co/gMoUl9zQ2Q\n",
      "#sismo detectado #jap_n 15:41:07 seismic intensity 0 iwate miyagi jst #?? \n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][38])\n",
    "print(train_df[\"text_clean\"][38])\n",
    "print(train_df[\"text\"][7586])\n",
    "print(train_df[\"text_clean\"][7586])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Remove_special_characters\"></a>\n",
    "\n",
    "### Remove special characters: \n",
    "The special characters could be symbols, emojis, and other graphic characters.\n",
    "We use the \"Toxic Comment Classification Challenge\" dataset as the \"Real or Not? NLP with Disaster Tweets\" dataset do not have any special charaters in their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_jtcc = pd.read_csv(r\"D:\\Sharif University of Tech\\Data\\NLP Feature Extraction\\Data\\jigsaw-toxic-comment-classification-challenge\\train.csv.zip\")\n",
    "print(train_df_jtcc.shape)\n",
    "train_df_jtcc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Explanation\\nWhy the edits made under my usern...  \n",
       "1  D'aww! He matches this background colour I'm s...  \n",
       "2  Hey man, I'm really not trying to edit war. It...  \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...  \n",
       "4  You, sir, are my hero. Any chance you remember...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove non-ascii characters from the text\n",
    "train_df_jtcc[\"text_clean\"] = train_df_jtcc[\"comment_text\"].apply(lambda x: remove_special_characters(x))\n",
    "display(train_df_jtcc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"P.S. It's not polite to talk to people behind their backs, please remove your comments from Mrph's talk page.\n",
      "\n",
      "Vaughan\n",
      "You're right; I went to check your previous edit and found a page on the Marvel site that spelled it \"\"Vaughn\"\", but now I am finding many more that spell it correctly. Thanks for the edits.   (☎☓) \n",
      "\n",
      "\"\n",
      "\"P.S. It's not polite to talk to people behind their backs, please remove your comments from Mrph's talk page.\n",
      "\n",
      "Vaughan\n",
      "You're right; I went to check your previous edit and found a page on the Marvel site that spelled it \"\"Vaughn\"\", but now I am finding many more that spell it correctly. Thanks for the edits.   () \n",
      "\n",
      "\"\n",
      "\"\n",
      "\n",
      "Sorry to interrupt but I'm at 1200 edits now... the first 200 were likely just on my own pages and because I was asking for help so much so maybe just 1000... or maybe less... but it still kind of counts. ♥♥Amulet♥♥ \"\n",
      "\"\n",
      "\n",
      "Sorry to interrupt but I'm at 1200 edits now... the first 200 were likely just on my own pages and because I was asking for help so much so maybe just 1000... or maybe less... but it still kind of counts. Amulet \"\n"
     ]
    }
   ],
   "source": [
    "print(train_df_jtcc[\"comment_text\"][143])\n",
    "print(train_df_jtcc[\"text_clean\"][143])\n",
    "print(train_df_jtcc[\"comment_text\"][189])\n",
    "print(train_df_jtcc[\"text_clean\"][189])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df_jtcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Remove_punctuations\"></a>\n",
    "\n",
    "## Remove punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "rockyfire update  california hwy 20 closed in both directions due to lake county fire  cafire wildfires\n",
      "#??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ... http://t.co/5B7qT2YxdA\n",
      "    mh370 aircraft debris found on la reunion is from missing malaysia airlines  \n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][5])\n",
    "print(train_df[\"text_clean\"][5])\n",
    "print(train_df[\"text\"][7597])\n",
    "print(train_df[\"text_clean\"][7597])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Other_Manual_Text_Cleaning_Tasks\"></a>\n",
    "\n",
    "## Other Manual Text Cleaning Tasks: \n",
    "\n",
    "Other techniques could be considered and manually processed case by case: \n",
    "    - Replace the Unicode character with equivalent ASCII character (instead of removing)\n",
    "    - Replace the entity references with their actual symbols  instead of removing as HTML tags\n",
    "    - Replace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.\n",
    "    - List out all the hashtags/ usernames then replace with equivalent words\n",
    "    - Replace the emoticon/ emoji with equivalant word meaning such as \":)\" with \"smile\" \n",
    "    - Spelling correction\n",
    "\n",
    "<a id=\"Replace_Typos\"></a>\n",
    "### Replace the Typos, slang, acronyms or informal abbreviations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_clean(text):\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"€\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  \n",
      "            be right back with some sample photos I lov you. I need some  dollar  for tomorrow.\n",
      "            \n",
      "MH370: Intact part lifts odds plane glided not crashed into sea http://t.co/8pdnHH6tzH\n",
      "malaysia airlines flight 370 intact part lifts odds plane glided not crashed into sea \n",
      "@USAgov Koreans are performing hijacking of the Tokyo Olympic Games.https://t.co/APkSnpLXZj\n",
      "usa government koreans are performing hijacking of the tokyo olympic games\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"\n",
    "            brb with some sample ph0tos I lov u. I need some $ for 2mw.\n",
    "            \"\"\"\n",
    "print(\"Test: \", other_clean(test_text))\n",
    "\n",
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: other_clean(x))\n",
    "\n",
    "print(train_df[\"text\"][1844])\n",
    "print(train_df[\"text_clean\"][1844])\n",
    "print(train_df[\"text\"][4409])\n",
    "print(train_df[\"text_clean\"][4409])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Spelling_correction\"></a>\n",
    "\n",
    "### Spelling Correction\n",
    "Spelling correction could also be considered an optional preprocessing task as the social media text data is often are typos or mistyped. However, the spelling correction output should be carefully double-checked with the original text input as it could be a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  sleepy and there is no place I'm going to.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "print(\"Test: \", TextBlob(\"sleapy and tehre is no plaxe I'm gioong to.\").correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Text_Preprocessing\"></a>\n",
    "\n",
    "# Text Preprocessing:\n",
    "\n",
    "<a id=\"Tokenization\"></a>\n",
    "## Tokenization\n",
    "Tokenization is a common technique that split a sentence into tokens, where a token could be characters, words, phrases, symbols, or other meaningful elements. By breaking sentences into smaller chunks, that would help to investigate the words in a sentence and also the subsequent steps in the NLP pipeline, such as stemming. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [our, deeds, are, the, reason, of, this, earth...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [all, residents, asked, to, shelter, in, place...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df['tokenized'] = train_df['text_clean'].apply(word_tokenize)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Remove_Stop_Words\"></a>\n",
    "\n",
    "## Remove Stop Words (or/and Frequent words/ Rare words):\n",
    "Stop words are common words in any language that occur with a high frequency but do not deliver meaningful information for the whole sentence. For example, {“a”, “about”, “above”, “across”, “after”, “afterward”, “again”, ...} can be considered as stop words. Traditionally, we could remove all of them in the text preprocessing stage. However, refer to the example from the [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action) book: \n",
    "> * Mark reported to the CEO\n",
    "> * Suzanne reported as the CEO to the board \n",
    "\n",
    "In your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to \"reported CEO\", and you would lack the information about the professional hierarchy. In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\n",
    "\n",
    "Designing a filter for stop words depends on your particular application.\n",
    "\n",
    "In short, removing stop words is a common method in NLP text preprocessing, whereas, it needs to be experimented carefully depending on different situations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets start by removing tthe stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [residents, asked, shelter, place, notified, o...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "train_df['stopwords_removed'] = train_df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Stemming\"></a>\n",
    "\n",
    "## Stemming\n",
    "Stemming is a process of extracting a root word - identifying a common stem among various forms (e.g., singular and plural noun form) of a word, for example, the words \"gardening\", \"gardener\" or \"gardens\" share the same stem, garden. Stemming uproots suffixes from words to merge words with similar meanings under their standard stem.\n",
    "\n",
    "There are three major stemming algorithms in use nowadays:\n",
    "- **Porter** - PorterStemmer()): This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It’s not too complex and development on it is frozen. Typically, it’s a nice starting basic stemmer, but it’s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others.\n",
    "\n",
    "- **Snowball** - LancasterStemmer(): This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter.\n",
    "\n",
    "- **Lancaster** - SnowballStemmer(): Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It’s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"PorterStemmer\"></a>\n",
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]  \n",
       "1       [forest, fire, near, la, rong, sask, canada]  \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...  \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['porter_stemmer'] = train_df['stopwords_removed'].apply(lambda x: porter_stemmer(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"SnowballStemmer\"></a>\n",
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def snowball_stemmer(text):\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                    snowball_stemmer  \n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]  \n",
       "1       [forest, fire, near, la, rong, sask, canada]  \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...  \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['snowball_stemmer'] = train_df['stopwords_removed'].apply(lambda x: snowball_stemmer(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"LancasterStemmer\"></a>\n",
    "### LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def lancaster_stemmer(text):\n",
    "    stemmer = nltk.LancasterStemmer()\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[dee, reason, earthquak, may, allah, forg, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fir, near, la, rong, sask, canad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelt, plac, not, off, evacu, she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, ord, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, ruby, alask, smok, wildfir,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   lancaster_stemmer  \n",
       "0     [dee, reason, earthquak, may, allah, forg, us]  \n",
       "1         [forest, fir, near, la, rong, sask, canad]  \n",
       "2  [resid, ask, shelt, plac, not, off, evacu, she...  \n",
       "3  [13000, peopl, receiv, wildfir, evacu, ord, ca...  \n",
       "4  [got, sent, photo, ruby, alask, smok, wildfir,...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['lancaster_stemmer'] = train_df['stopwords_removed'].apply(lambda x: lancaster_stemmer(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"POS_Tagging\"></a>\n",
    "\n",
    "## Part of Speech Tagging (POS Tagging):\n",
    "Part of speech tagging (POS tagging) distinguishes the part of speech (noun, verb, adjective, and etc.) of each word in the text. This is the critical stage for many NLP applications since, by identifying the POS of a word, we can infer its contextual meaning. The NLTK packages offer different POS Tagging algorithms, and in this notebook, we use the combination version of them.\n",
    "\n",
    "- pos_tag/ DefaultTagger\n",
    "- UnigramTagger\n",
    "- BigramTagger\n",
    "- Could also be a combination of the bigram tagger, unigram tagger, and default tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pos_tag with wordnet format and also map the pos tagging output with wordnet output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('residents', 'n'),\n",
       " ('asked', 'v'),\n",
       " ('shelter', 'n'),\n",
       " ('place', 'n'),\n",
       " ('notified', 'n'),\n",
       " ('officers', 'n'),\n",
       " ('evacuation', 'n'),\n",
       " ('shelter', 'n'),\n",
       " ('place', 'n'),\n",
       " ('orders', 'n'),\n",
       " ('expected', 'v')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_wordnet(train_df['stopwords_removed'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[dee, reason, earthquak, may, allah, forg, us]</td>\n",
       "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fir, near, la, rong, sask, canad]</td>\n",
       "      <td>[(forest, n), (fire, n), (near, n), (la, n), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelt, plac, not, off, evacu, she...</td>\n",
       "      <td>[(residents, n), (asked, v), (shelter, n), (pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, ord, ca...</td>\n",
       "      <td>[(13000, n), (people, n), (receive, v), (wildf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, ruby, alask, smok, wildfir,...</td>\n",
       "      <td>[(got, v), (sent, v), (photo, n), (ruby, n), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0     [dee, reason, earthquak, may, allah, forg, us]   \n",
       "1         [forest, fir, near, la, rong, sask, canad]   \n",
       "2  [resid, ask, shelt, plac, not, off, evacu, she...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, ord, ca...   \n",
       "4  [got, sent, photo, ruby, alask, smok, wildfir,...   \n",
       "\n",
       "                                combined_postag_wnet  \n",
       "0  [(deeds, n), (reason, n), (earthquake, n), (ma...  \n",
       "1  [(forest, n), (fire, n), (near, n), (la, n), (...  \n",
       "2  [(residents, n), (asked, v), (shelter, n), (pl...  \n",
       "3  [(13000, n), (people, n), (receive, v), (wildf...  \n",
       "4  [(got, v), (sent, v), (photo, n), (ruby, n), (...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['combined_postag_wnet'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Lemmatization\"></a>\n",
    "\n",
    "## Lemmatization:\n",
    "According to the [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) book:\n",
    "> Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us ﬁnd all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.\n",
    "\n",
    "and the book [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action):\n",
    "> Some lemmatizers use the word’s part of speech (POS) tag in addition to its spelling to help improve accuracy. The POS tag for a word indicates its role in the grammar of a phrase or sentence. For example, the noun POS is for words that refer to “people, places, or things” within a phrase. An adjective POS is for a word that modifies or describes a noun. A verb refers to an action. The POS of a word in isolation cannot be determined. The context of a word must be known for its POS to be identified. So some advanced lemmatizers can’t be run-on words in isolation.\n",
    "\n",
    "For example, the \"good\", \"better\" or \"best\" is lemmatized into good and the verb \"gardening\" should be lemmatized to \"to garden\", while the \"garden\" and \"gardener\" are both different lemmas. In this notebook, we will also explore on both lemmatize on without POS-Tagging and POS-Tagging examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Lemmatization_wo_pos\"></a>\n",
    "\n",
    "### Lemmatization without POS Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[dee, reason, earthquak, may, allah, forg, us]</td>\n",
       "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fir, near, la, rong, sask, canad]</td>\n",
       "      <td>[(forest, n), (fire, n), (near, n), (la, n), (...</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelt, plac, not, off, evacu, she...</td>\n",
       "      <td>[(residents, n), (asked, v), (shelter, n), (pl...</td>\n",
       "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, ord, ca...</td>\n",
       "      <td>[(13000, n), (people, n), (receive, v), (wildf...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, ruby, alask, smok, wildfir,...</td>\n",
       "      <td>[(got, v), (sent, v), (photo, n), (ruby, n), (...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0     [dee, reason, earthquak, may, allah, forg, us]   \n",
       "1         [forest, fir, near, la, rong, sask, canad]   \n",
       "2  [resid, ask, shelt, plac, not, off, evacu, she...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, ord, ca...   \n",
       "4  [got, sent, photo, ruby, alask, smok, wildfir,...   \n",
       "\n",
       "                                combined_postag_wnet  \\\n",
       "0  [(deeds, n), (reason, n), (earthquake, n), (ma...   \n",
       "1  [(forest, n), (fire, n), (near, n), (la, n), (...   \n",
       "2  [(residents, n), (asked, v), (shelter, n), (pl...   \n",
       "3  [(13000, n), (people, n), (receive, v), (wildf...   \n",
       "4  [(got, v), (sent, v), (photo, n), (ruby, n), (...   \n",
       "\n",
       "                               lemmatize_word_wo_pos  \n",
       "0  [deed, reason, earthquake, may, allah, forgive...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [resident, asked, shelter, place, notified, of...  \n",
       "3  [13000, people, receive, wildfire, evacuation,...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['lemmatize_word_wo_pos'] = train_df['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "train_df['lemmatize_word_wo_pos'] = train_df['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emergency', 'n'), ('evacuation', 'n'), ('happening', 'v'), ('building', 'n'), ('across', 'n'), ('street', 'n')]\n",
      "['emergency', 'evacuation', 'happening', 'building', 'across', 'street']\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"combined_postag_wnet\"][8])\n",
    "print(train_df[\"lemmatize_word_wo_pos\"][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Lemmatization_w_pos\"></a>\n",
    "\n",
    "### Lemmatization with POS Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>snowball_stemmer</th>\n",
       "      <th>lancaster_stemmer</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "      <th>lemmatize_word_wo_pos</th>\n",
       "      <th>lemmatize_word_w_pos</th>\n",
       "      <th>lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[dee, reason, earthquak, may, allah, forg, us]</td>\n",
       "      <td>[(deeds, n), (reason, n), (earthquake, n), (ma...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fir, near, la, rong, sask, canad]</td>\n",
       "      <td>[(forest, n), (fire, n), (near, n), (la, n), (...</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resid, ask, shelt, plac, not, off, evacu, she...</td>\n",
       "      <td>[(residents, n), (asked, v), (shelter, n), (pl...</td>\n",
       "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
       "      <td>[resident, ask, shelter, place, notified, offi...</td>\n",
       "      <td>resident ask shelter place notified officer ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, ord, ca...</td>\n",
       "      <td>[(13000, n), (people, n), (receive, v), (wildf...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, ruby, alask, smok, wildfir,...</td>\n",
       "      <td>[(got, v), (sent, v), (photo, n), (ruby, n), (...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                    snowball_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   lancaster_stemmer  \\\n",
       "0     [dee, reason, earthquak, may, allah, forg, us]   \n",
       "1         [forest, fir, near, la, rong, sask, canad]   \n",
       "2  [resid, ask, shelt, plac, not, off, evacu, she...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, ord, ca...   \n",
       "4  [got, sent, photo, ruby, alask, smok, wildfir,...   \n",
       "\n",
       "                                combined_postag_wnet  \\\n",
       "0  [(deeds, n), (reason, n), (earthquake, n), (ma...   \n",
       "1  [(forest, n), (fire, n), (near, n), (la, n), (...   \n",
       "2  [(residents, n), (asked, v), (shelter, n), (pl...   \n",
       "3  [(13000, n), (people, n), (receive, v), (wildf...   \n",
       "4  [(got, v), (sent, v), (photo, n), (ruby, n), (...   \n",
       "\n",
       "                               lemmatize_word_wo_pos  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [resident, asked, shelter, place, notified, of...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                lemmatize_word_w_pos  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [resident, ask, shelter, place, notified, offi...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [get, send, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      lemmatize_text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notified officer ev...  \n",
       "3  13000 people receive wildfire evacuation order...  \n",
       "4  get send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['lemmatize_word_w_pos'] = train_df['combined_postag_wnet'].apply(lambda x: lemmatize_word(x))\n",
    "train_df['lemmatize_word_w_pos'] = train_df['lemmatize_word_w_pos'].apply(lambda x: [word for word in x if word not in stop]) # double check to remove stop words\n",
    "train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word_w_pos']] # join back to text\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the output of Lemmatization on non-POS-Tagging and POS-Tagging output. We can see in the original text, the word \\\" happening\\\" is a verb and was corrected assigned as a verb by POS-tagging stage, then Lemmatize accurately with back as \\\"happen\\\" but lemmatized without-POS-tagging resulted in \\\"happening\\\" is not correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's an emergency evacuation happening now in the building across the street\n",
      "[('emergency', 'n'), ('evacuation', 'n'), ('happening', 'v'), ('building', 'n'), ('across', 'n'), ('street', 'n')]\n",
      "['emergency', 'evacuation', 'happening', 'building', 'across', 'street']\n",
      "['emergency', 'evacuation', 'happen', 'building', 'across', 'street']\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"text\"][8])\n",
    "print(train_df[\"combined_postag_wnet\"][8])\n",
    "print(train_df[\"lemmatize_word_wo_pos\"][8])\n",
    "print(train_df[\"lemmatize_word_w_pos\"][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between original text and the lammatized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'deed reason earthquake may allah forgive u'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'rockyfire update california hwy 20 close direction due lake county fire cafire wildfire'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Three people died from the heat wave so far'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'three people die heat wave far'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"What's up man?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'man'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'this is ridiculous....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ridiculous'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df[\"text\"][0], train_df[\"lemmatize_text\"][0])\n",
    "display(train_df[\"text\"][5], train_df[\"lemmatize_text\"][5])\n",
    "display(train_df[\"text\"][10], train_df[\"lemmatize_text\"][10])\n",
    "display(train_df[\"text\"][15], train_df[\"lemmatize_text\"][15])\n",
    "display(train_df[\"text\"][20], train_df[\"lemmatize_text\"][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Other_Text_Preprocessing\"></a>\n",
    "\n",
    "## Other (Optional) Text Preprocessing Techniques:\n",
    "- language detection\n",
    "- Code mixing and transliteration\n",
    "\n",
    "<a id=\"Language_Detection\"></a>\n",
    "### Language Detection:\n",
    "We will use the package [polyglot](https://github.com/aboSamoor/polyglot) for language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the \"Jigsaw Multilingual Toxic Comment Classification\" dataset for this case as the dataset is multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_jmtc = pd.read_csv(r\"D:\\Sharif University of Tech\\Data\\NLP Feature Extraction\\Data\\jigsaw-multilingual-toxic-comment-classification\\jigsaw-toxic-comment-train.csv\")\n",
    "# print(train_df_jmtc.shape)\n",
    "# train_df_jmtc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from polyglot.detect import Detector\n",
    "\n",
    "# def get_language(text):\n",
    "#     return Detector(\"\".join(x for x in text if x.isprintable()), quiet=True).languages[0].name\n",
    "\n",
    "# train_df_jmtc[\"lang\"] = train_df_jmtc[\"comment_text\"].apply(lambda x: get_language(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(train_df_jmtc[train_df_jmtc[\"lang\"] == \"de\"].head())\n",
    "# print(train_df_jmtc[\"comment_text\"][823])\n",
    "# print(train_df_jmtc[\"comment_text\"][8130])\n",
    "# print(train_df_jmtc[\"comment_text\"][14511])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Text_Features_Extraction\"></a>\n",
    "\n",
    "# Text Features Extraction:\n",
    "\n",
    "<a id=\"BoW\"></a>\n",
    "## Weighted Words - Bag of Words (BoW) - Bag of n-grams:\n",
    "* N-gram is a sequence that contains n-elements (characters, words, etc). A single word such a \"apple\", \"orange\" is a Uni-gram; hence, \"red apple\" \"big orange\" is bi-gram and \"red ripped apple\", \"big orange bag\" is tri-gram. \n",
    "* Bags of words: Vectors of word counts or frequencies \n",
    "* Bags of n-grams: Counts of word pairs (bigrams), triplets (trigrams), and so on\n",
    "\n",
    "> The bag-of-words/ bag-of-n-gram model is a reduced and simpliﬁed representation of a text document from selected parts of the text, based on speciﬁc criteria, such as word frequency.\n",
    "> \n",
    "> In a BoW, a body of text, such as a document or a sentence, is thought of like a bag of words. Lists of words are created in the BoW process. These words in a matrix are not sentences which structure sentences and grammar, and the semantic relationship between these words are ignored in their collection and construction. The words are often representative of the content of a sentence. While grammar and order of appearance are ignored, multiplicity is counted and may be used later to determine the focus points of the documents.\n",
    "> \n",
    "> Example:\n",
    "> Document\n",
    "> \n",
    "> “As the home to UVA’s recognized undergraduate and graduate degree programs in systems engineering. In the UVA Department of Systems and Information Engineering, our students are exposed to a wide range of range”\n",
    "> \n",
    "> Bag-of-Words (BoW):\n",
    "> {“As”, “the”, “home”, “to”, “UVA’s”, “recognized”, “undergraduate”, “and”, “graduate”, “degree”, “program”, “in”, “systems”, “engineering”, “in”, “Department”, “Information”,“students”, “ ”,“are”, “exposed”, “wide”, “range” }\n",
    "> \n",
    "> Bag-of-Feature (BoF)\n",
    "> Feature = {1,1,1,3,2,1,2,1,2,3,1,1,1,2,1,1,1,1,1,1}\n",
    "\n",
    "(source:[Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
    "\n",
    "<a id=\"CountVectorizer\"></a>\n",
    "### Frequency Vectors - CountVectorizer:\n",
    "We will implement the Bag of Words/ Bag of n-grams text representation via sklearn - CountVectorizer function.\n",
    "The code will test with a sample corpus of the first five sentence of the dataset, then print out the output of uni-gram, bi-gram and tri-gram. Finaly, we also run on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def cv(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
    "    count_vectorizer = CountVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n",
    "    emb = count_vectorizer.fit_transform(data).toarray()\n",
    "    print(\"count vectorize with\", str(np.array(emb).shape[1]), \"features\")\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(emb, feat, ngram, compared_sentence=0):\n",
    "    print(ngram,\"bag-of-words: \")\n",
    "    print(feat.get_feature_names_out(), \"\\n\") \n",
    "    print(ngram,\"bag-of-feature: \")\n",
    "    print(feat.vocabulary_, \"\\n\") \n",
    "    print(\"BoW matrix:\")\n",
    "    print(pd.DataFrame(emb.transpose(), index=feat.get_feature_names_out()).head(), \"\\n\")\n",
    "    print(ngram,\"vector example:\")\n",
    "    print(train_df[\"lemmatize_text\"][compared_sentence])\n",
    "    print(emb[compared_sentence], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test corpus:  ['deed reason earthquake may allah forgive u', 'forest fire near la ronge sask canada', 'resident ask shelter place notified officer evacuation shelter place order expect', '13000 people receive wildfire evacuation order california', 'get send photo ruby alaska smoke wildfire pours school'] \n",
      "\n",
      "count vectorize with 35 features\n",
      "Uni-gram bag-of-words: \n",
      "['13000' 'alaska' 'allah' 'ask' 'california' 'canada' 'deed' 'earthquake'\n",
      " 'evacuation' 'expect' 'fire' 'forest' 'forgive' 'get' 'la' 'may' 'near'\n",
      " 'notified' 'officer' 'order' 'people' 'photo' 'place' 'pours' 'reason'\n",
      " 'receive' 'resident' 'ronge' 'ruby' 'sask' 'school' 'send' 'shelter'\n",
      " 'smoke' 'wildfire'] \n",
      "\n",
      "Uni-gram bag-of-feature: \n",
      "{'deed': 6, 'reason': 24, 'earthquake': 7, 'may': 15, 'allah': 2, 'forgive': 12, 'forest': 11, 'fire': 10, 'near': 16, 'la': 14, 'ronge': 27, 'sask': 29, 'canada': 5, 'resident': 26, 'ask': 3, 'shelter': 32, 'place': 22, 'notified': 17, 'officer': 18, 'evacuation': 8, 'order': 19, 'expect': 9, '13000': 0, 'people': 20, 'receive': 25, 'wildfire': 34, 'california': 4, 'get': 13, 'send': 31, 'photo': 21, 'ruby': 28, 'alaska': 1, 'smoke': 33, 'pours': 23, 'school': 30} \n",
      "\n",
      "BoW matrix:\n",
      "            0  1  2  3  4\n",
      "13000       0  0  0  1  0\n",
      "alaska      0  0  0  0  1\n",
      "allah       1  0  0  0  0\n",
      "ask         0  0  1  0  0\n",
      "california  0  0  0  1  0 \n",
      "\n",
      "Uni-gram vector example:\n",
      "deed reason earthquake may allah forgive u\n",
      "[0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corpus = train_df[\"lemmatize_text\"][:5].tolist()\n",
    "print(\"The test corpus: \", test_corpus, \"\\n\")\n",
    "\n",
    "test_cv_em_1gram, test_cv_1gram = cv(test_corpus, ngram=1)\n",
    "print_out(test_cv_em_1gram, test_cv_1gram, ngram=\"Uni-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectorize with 34 features\n",
      "Bi-gram bag-of-words: \n",
      "['13000 people' 'alaska smoke' 'allah forgive' 'ask shelter' 'deed reason'\n",
      " 'earthquake may' 'evacuation order' 'evacuation shelter' 'fire near'\n",
      " 'forest fire' 'get send' 'la ronge' 'may allah' 'near la'\n",
      " 'notified officer' 'officer evacuation' 'order california' 'order expect'\n",
      " 'people receive' 'photo ruby' 'place notified' 'place order'\n",
      " 'pours school' 'reason earthquake' 'receive wildfire' 'resident ask'\n",
      " 'ronge sask' 'ruby alaska' 'sask canada' 'send photo' 'shelter place'\n",
      " 'smoke wildfire' 'wildfire evacuation' 'wildfire pours'] \n",
      "\n",
      "Bi-gram bag-of-feature: \n",
      "{'deed reason': 4, 'reason earthquake': 23, 'earthquake may': 5, 'may allah': 12, 'allah forgive': 2, 'forest fire': 9, 'fire near': 8, 'near la': 13, 'la ronge': 11, 'ronge sask': 26, 'sask canada': 28, 'resident ask': 25, 'ask shelter': 3, 'shelter place': 30, 'place notified': 20, 'notified officer': 14, 'officer evacuation': 15, 'evacuation shelter': 7, 'place order': 21, 'order expect': 17, '13000 people': 0, 'people receive': 18, 'receive wildfire': 24, 'wildfire evacuation': 32, 'evacuation order': 6, 'order california': 16, 'get send': 10, 'send photo': 29, 'photo ruby': 19, 'ruby alaska': 27, 'alaska smoke': 1, 'smoke wildfire': 31, 'wildfire pours': 33, 'pours school': 22} \n",
      "\n",
      "BoW matrix:\n",
      "               0  1  2  3  4\n",
      "13000 people   0  0  0  1  0\n",
      "alaska smoke   0  0  0  0  1\n",
      "allah forgive  1  0  0  0  0\n",
      "ask shelter    0  0  1  0  0\n",
      "deed reason    1  0  0  0  0 \n",
      "\n",
      "Bi-gram vector example:\n",
      "deed reason earthquake may allah forgive u\n",
      "[0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cv_em_2gram, test_cv_2gram = cv(test_corpus, ngram=2)\n",
    "print_out(test_cv_em_2gram, test_cv_2gram, ngram=\"Bi-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectorize with 30 features\n",
      "Tri-gram bag-of-words: \n",
      "['13000 people' 'alaska smoke' 'allah forgive' 'ask shelter' 'deed reason'\n",
      " 'earthquake may' 'evacuation order' 'evacuation shelter' 'fire near'\n",
      " 'forest fire' 'get send' 'la ronge' 'may allah' 'near la'\n",
      " 'notified officer' 'officer evacuation' 'order california' 'order expect'\n",
      " 'people receive' 'photo ruby' 'place notified' 'place order'\n",
      " 'pours school' 'reason earthquake' 'receive wildfire' 'resident ask'\n",
      " 'ronge sask' 'ruby alaska' 'sask canada' 'send photo' 'shelter place'\n",
      " 'smoke wildfire' 'wildfire evacuation' 'wildfire pours'] \n",
      "\n",
      "Tri-gram bag-of-feature: \n",
      "{'deed reason': 4, 'reason earthquake': 23, 'earthquake may': 5, 'may allah': 12, 'allah forgive': 2, 'forest fire': 9, 'fire near': 8, 'near la': 13, 'la ronge': 11, 'ronge sask': 26, 'sask canada': 28, 'resident ask': 25, 'ask shelter': 3, 'shelter place': 30, 'place notified': 20, 'notified officer': 14, 'officer evacuation': 15, 'evacuation shelter': 7, 'place order': 21, 'order expect': 17, '13000 people': 0, 'people receive': 18, 'receive wildfire': 24, 'wildfire evacuation': 32, 'evacuation order': 6, 'order california': 16, 'get send': 10, 'send photo': 29, 'photo ruby': 19, 'ruby alaska': 27, 'alaska smoke': 1, 'smoke wildfire': 31, 'wildfire pours': 33, 'pours school': 22} \n",
      "\n",
      "BoW matrix:\n",
      "               0  1  2  3  4\n",
      "13000 people   0  0  0  1  0\n",
      "alaska smoke   0  0  0  0  1\n",
      "allah forgive  1  0  0  0  0\n",
      "ask shelter    0  0  1  0  0\n",
      "deed reason    1  0  0  0  0 \n",
      "\n",
      "Tri-gram vector example:\n",
      "deed reason earthquake may allah forgive u\n",
      "[0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cv_em_3gram, test_cv_3gram = cv(test_corpus, ngram=3)\n",
    "print_out(test_cv_em_2gram, test_cv_2gram, ngram=\"Tri-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vectorize with 15514 features\n",
      "count vectorize with 45510 features\n",
      "count vectorize with 43696 features\n",
      "7613\n",
      "(7613, 15514)\n",
      "(7613, 45510)\n",
      "(7613, 43696)\n"
     ]
    }
   ],
   "source": [
    "train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n",
    "train_df_em_1gram, vc_1gram = cv(train_df_corpus, 1)\n",
    "train_df_em_2gram, vc_2gram = cv(train_df_corpus, 2)\n",
    "train_df_em_3gram, vc_3gram = cv(train_df_corpus, 3)\n",
    "\n",
    "print(len(train_df_corpus))\n",
    "print(train_df_em_1gram.shape)\n",
    "print(train_df_em_2gram.shape)\n",
    "print(train_df_em_3gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df_em_1gram, train_df_em_2gram, train_df_em_3gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TF_IDF\"></a>\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency (TF-IDF):\n",
    "> The Inverse Document Frequency (IDF) as a method to be used in conjunction with term frequency in order to lessen the effect of implicitly common words in the corpus. IDF assigns a higher weight to words with either high or low frequencies term in the document. This combination of TF and IDF is well known as Term Frequency-Inverse document frequency (TF-IDF). The mathematical representation of the weight of a term in a document by TF-IDF is given in Equation: \n",
    "> $$ W(d,t) = TF(d,t) * log \\frac{N}{df(t)}$$\n",
    "> Here N is the number of documents and $df(t)$ is the number of documents containing the term t in the corpus. The ﬁrst term in the equation improves the recall while the second term improves the precision of the word embedding. Although TF-IDF tries to overcome the problem of common terms in the document, it still suffers from some other descriptive limitations. Namely, TF-IDF cannot account for the similarity between the words in the document since each word is independently presented as an index. However, with the development of more complex models in recent years, new methods, such as word embedding, have been presented that can incorporate concepts such as similarity of words and part of speech tagging.\n",
    "\n",
    "(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
    "\n",
    "We also implement the TF-IDF via sklearn TfidfVectorizer function, the experiments are similar to the previous [Frequency Vectors - CountVectorizer](#CountVectorizer) section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def TFIDF(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
    "    tfidf_x = TfidfVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n",
    "    emb = tfidf_x.fit_transform(data).toarray()\n",
    "    print(\"tf-idf with\", str(np.array(emb).shape[1]), \"features\")\n",
    "    return emb, tfidf_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test corpus:  ['deed reason earthquake may allah forgive u', 'forest fire near la ronge sask canada', 'resident ask shelter place notified officer evacuation shelter place order expect', '13000 people receive wildfire evacuation order california', 'get send photo ruby alaska smoke wildfire pours school'] \n",
      "\n",
      "tf-idf with 35 features\n",
      "Uni-gram bag-of-words: \n",
      "['13000' 'alaska' 'allah' 'ask' 'california' 'canada' 'deed' 'earthquake'\n",
      " 'evacuation' 'expect' 'fire' 'forest' 'forgive' 'get' 'la' 'may' 'near'\n",
      " 'notified' 'officer' 'order' 'people' 'photo' 'place' 'pours' 'reason'\n",
      " 'receive' 'resident' 'ronge' 'ruby' 'sask' 'school' 'send' 'shelter'\n",
      " 'smoke' 'wildfire'] \n",
      "\n",
      "Uni-gram bag-of-feature: \n",
      "{'deed': 6, 'reason': 24, 'earthquake': 7, 'may': 15, 'allah': 2, 'forgive': 12, 'forest': 11, 'fire': 10, 'near': 16, 'la': 14, 'ronge': 27, 'sask': 29, 'canada': 5, 'resident': 26, 'ask': 3, 'shelter': 32, 'place': 22, 'notified': 17, 'officer': 18, 'evacuation': 8, 'order': 19, 'expect': 9, '13000': 0, 'people': 20, 'receive': 25, 'wildfire': 34, 'california': 4, 'get': 13, 'send': 31, 'photo': 21, 'ruby': 28, 'alaska': 1, 'smoke': 33, 'pours': 23, 'school': 30} \n",
      "\n",
      "BoW matrix:\n",
      "                   0    1         2         3         4\n",
      "13000       0.000000  0.0  0.000000  0.409865  0.000000\n",
      "alaska      0.000000  0.0  0.000000  0.000000  0.339992\n",
      "allah       0.408248  0.0  0.000000  0.000000  0.000000\n",
      "ask         0.000000  0.0  0.264426  0.000000  0.000000\n",
      "california  0.000000  0.0  0.000000  0.409865  0.000000 \n",
      "\n",
      "Uni-gram vector example:\n",
      "deed reason earthquake may allah forgive u\n",
      "[0.         0.         0.40824829 0.         0.         0.\n",
      " 0.40824829 0.40824829 0.         0.         0.         0.\n",
      " 0.40824829 0.         0.         0.40824829 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.40824829 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corpus = train_df[\"lemmatize_text\"][:5].tolist()\n",
    "print(\"The test corpus: \", test_corpus, \"\\n\")\n",
    "\n",
    "test_tfidf_em_1gram, test_tfidf_1gram = TFIDF(test_corpus, ngram=1)\n",
    "print_out(test_tfidf_em_1gram, test_tfidf_1gram, ngram=\"Uni-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 34 features\n",
      "Bi-gram bag-of-words: \n",
      "['13000 people' 'alaska smoke' 'allah forgive' 'ask shelter' 'deed reason'\n",
      " 'earthquake may' 'evacuation order' 'evacuation shelter' 'fire near'\n",
      " 'forest fire' 'get send' 'la ronge' 'may allah' 'near la'\n",
      " 'notified officer' 'officer evacuation' 'order california' 'order expect'\n",
      " 'people receive' 'photo ruby' 'place notified' 'place order'\n",
      " 'pours school' 'reason earthquake' 'receive wildfire' 'resident ask'\n",
      " 'ronge sask' 'ruby alaska' 'sask canada' 'send photo' 'shelter place'\n",
      " 'smoke wildfire' 'wildfire evacuation' 'wildfire pours'] \n",
      "\n",
      "Bi-gram bag-of-feature: \n",
      "{'deed reason': 4, 'reason earthquake': 23, 'earthquake may': 5, 'may allah': 12, 'allah forgive': 2, 'forest fire': 9, 'fire near': 8, 'near la': 13, 'la ronge': 11, 'ronge sask': 26, 'sask canada': 28, 'resident ask': 25, 'ask shelter': 3, 'shelter place': 30, 'place notified': 20, 'notified officer': 14, 'officer evacuation': 15, 'evacuation shelter': 7, 'place order': 21, 'order expect': 17, '13000 people': 0, 'people receive': 18, 'receive wildfire': 24, 'wildfire evacuation': 32, 'evacuation order': 6, 'order california': 16, 'get send': 10, 'send photo': 29, 'photo ruby': 19, 'ruby alaska': 27, 'alaska smoke': 1, 'smoke wildfire': 31, 'wildfire pours': 33, 'pours school': 22} \n",
      "\n",
      "BoW matrix:\n",
      "                      0    1         2         3         4\n",
      "13000 people   0.000000  0.0  0.000000  0.408248  0.000000\n",
      "alaska smoke   0.000000  0.0  0.000000  0.000000  0.353553\n",
      "allah forgive  0.447214  0.0  0.000000  0.000000  0.000000\n",
      "ask shelter    0.000000  0.0  0.288675  0.000000  0.000000\n",
      "deed reason    0.447214  0.0  0.000000  0.000000  0.000000 \n",
      "\n",
      "Bi-gram vector example:\n",
      "deed reason earthquake may allah forgive u\n",
      "[0.        0.        0.4472136 0.        0.4472136 0.4472136 0.\n",
      " 0.        0.        0.        0.        0.        0.4472136 0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.4472136 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tfidf_em_2gram, test_tfidf_2gram = TFIDF(test_corpus, ngram=2)\n",
    "print_out(test_tfidf_em_2gram, test_tfidf_2gram, ngram=\"Bi-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 30 features\n",
      "Tri-gram bag-of-words: \n",
      "['13000 people receive' 'alaska smoke wildfire' 'ask shelter place'\n",
      " 'deed reason earthquake' 'earthquake may allah'\n",
      " 'evacuation order california' 'evacuation shelter place' 'fire near la'\n",
      " 'forest fire near' 'get send photo' 'la ronge sask' 'may allah forgive'\n",
      " 'near la ronge' 'notified officer evacuation'\n",
      " 'officer evacuation shelter' 'people receive wildfire'\n",
      " 'photo ruby alaska' 'place notified officer' 'place order expect'\n",
      " 'reason earthquake may' 'receive wildfire evacuation'\n",
      " 'resident ask shelter' 'ronge sask canada' 'ruby alaska smoke'\n",
      " 'send photo ruby' 'shelter place notified' 'shelter place order'\n",
      " 'smoke wildfire pours' 'wildfire evacuation order'\n",
      " 'wildfire pours school'] \n",
      "\n",
      "Tri-gram bag-of-feature: \n",
      "{'deed reason earthquake': 3, 'reason earthquake may': 19, 'earthquake may allah': 4, 'may allah forgive': 11, 'forest fire near': 8, 'fire near la': 7, 'near la ronge': 12, 'la ronge sask': 10, 'ronge sask canada': 22, 'resident ask shelter': 21, 'ask shelter place': 2, 'shelter place notified': 25, 'place notified officer': 17, 'notified officer evacuation': 13, 'officer evacuation shelter': 14, 'evacuation shelter place': 6, 'shelter place order': 26, 'place order expect': 18, '13000 people receive': 0, 'people receive wildfire': 15, 'receive wildfire evacuation': 20, 'wildfire evacuation order': 28, 'evacuation order california': 5, 'get send photo': 9, 'send photo ruby': 24, 'photo ruby alaska': 16, 'ruby alaska smoke': 23, 'alaska smoke wildfire': 1, 'smoke wildfire pours': 27, 'wildfire pours school': 29} \n",
      "\n",
      "BoW matrix:\n",
      "                          0    1         2         3         4\n",
      "13000 people receive    0.0  0.0  0.000000  0.447214  0.000000\n",
      "alaska smoke wildfire   0.0  0.0  0.000000  0.000000  0.377964\n",
      "ask shelter place       0.0  0.0  0.333333  0.000000  0.000000\n",
      "deed reason earthquake  0.5  0.0  0.000000  0.000000  0.000000\n",
      "earthquake may allah    0.5  0.0  0.000000  0.000000  0.000000 \n",
      "\n",
      "Tri-gram vector example:\n",
      "deed reason earthquake may allah forgive u\n",
      "[0.  0.  0.  0.5 0.5 0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.\n",
      " 0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tfidf_em_3gram, test_tfidf_3gram = TFIDF(test_corpus, ngram=3)\n",
    "print_out(test_tfidf_em_3gram, test_tfidf_3gram, ngram=\"Tri-gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 15514 features\n",
      "tf-idf with 45510 features\n",
      "tf-idf with 43696 features\n",
      "7613\n",
      "(7613, 15514)\n",
      "(7613, 15514)\n",
      "(7613, 15514)\n"
     ]
    }
   ],
   "source": [
    "train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n",
    "train_df_tfidf_1gram, tfidf_1gram = TFIDF(train_df_corpus, 1)\n",
    "train_df_tfidf_2gram, tfidf_2gram = TFIDF(train_df_corpus, 2)\n",
    "train_df_tfidf_3gram, tfidf_3gram = TFIDF(train_df_corpus, 3)\n",
    "\n",
    "print(len(train_df_corpus))\n",
    "print(train_df_tfidf_1gram.shape)\n",
    "print(train_df_tfidf_1gram.shape)\n",
    "print(train_df_tfidf_1gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df_tfidf_1gram, train_df_tfidf_2gram, train_df_tfidf_3gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Word_Embedding\"></a>\n",
    "\n",
    "## Word Embedding:\n",
    "\n",
    "> **Word vectors** are numerical vector representations of word semantics, or meaning, including literal and implied meaning. So word vectors can capture the connotation of words, like “peopleness,” “animalness,” “placeness,” “thingness,” and even “conceptness.” And they combine all that into a dense vector (no zeros) of floating point values. This dense vector enables queries and logical reasoning.\n",
    "\n",
    "(source: [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action))\n",
    "\n",
    "> Even though we have syntactic word representations, it does not mean that the model captures the semantics meaning of the words. On the other hand, bag-of-word models do not respect the semantics of the word. For example, words “airplane”, “aeroplane”, “plane”, and “aircraft” are often used in the same context. However, the vectors corresponding to these words are orthogonal in the bag-of-words model. This issue presents a serious problem to understanding sentences within the model. The other problem in the bag-of-word is that the order of words in the phrase is not respected. The n-gram does not solve this problem so a similarity needs to be found for each word in the sentence. Many researchers worked on word embedding to solve this problem. The Word2Vec propose a simple single-layer architecture based on the inner product between two word vectors.\n",
    "\n",
    "> Word embedding is a feature learning technique in which each word or phrase from the vocabulary is mapped to a N dimension vector of real numbers. Various word embedding methods have been proposed to translate unigrams into understandable input for machine learning algorithms. This work focuses on Word2Vec, GloVe, and FastText, three of the most common methods that have been successfully used for deep learning techniques.\n",
    "\n",
    "(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
    "\n",
    "<a id=\"Basic_Word_Embedding\"></a>\n",
    "### Basic Word Embedding Methods:\n",
    "\n",
    "<a id=\"Word2Vec\"></a>\n",
    "#### Word2Vec:\n",
    "\n",
    "[T. Mikolov et al.](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) presented the Word2vec in 2013, which learns the meaning of words merely by processing a large corpus of unlabeled text. The Word2Vec approach uses shallow neural networks with two hidden layers, continuous bag-of-words (CBOW), and the Skip-gram model to create a high dimension vector for each word. This unsupervised nature of Word2vec is what makes it so powerful. The world is full of unlabeled, uncategorized, unstructured natural language text.\n",
    "\n",
    "We will implement the Word2vec via gensim libary with the pre-trained word vectors on the dataset Google News corpus (source: https://code.google.com/archive/p/word2vec/) and see the embedding output on the sample sentence from the our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: 4.3.2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"gensim version:\", gensim.__version__)\n",
    "\n",
    "word2vec_path = r\"D:\\Sharif University of Tech\\Data\\NLP Feature Extraction\\Data\\googlenewsvectorsnegative300\\GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=200000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74649847\n",
      "0.8099379\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.similarity('cat', 'kitten'))\n",
    "print(word2vec_model.similarity('cat', 'cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vec(tokens_list, vector, generate_missing=False, k=300):    \n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    \n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_embeddings(vectors, text, generate_missing=False, k=300):\n",
    "    embeddings = text.apply(lambda x: get_average_vec(x, vectors, generate_missing=generate_missing, k=k))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size 7613 300\n",
      "The sentence: \"deed reason earthquake may allah forgive u\" got embedding values: \n",
      "[-0.10973104  0.09386771  0.02338518  0.08328102 -0.0219574   0.02885873\n",
      " -0.08006577 -0.04429408 -0.04224214  0.02420189 -0.03073424 -0.06804257\n",
      " -0.16214498  0.04603213 -0.10248093  0.07793608  0.0992388   0.11692011\n",
      " -0.01385716  0.01890346 -0.19027274 -0.03666796  0.06822132  0.0259938\n",
      " -0.06404114  0.01582078 -0.16466123  0.02864947 -0.00298782 -0.0321074\n",
      " -0.03302729  0.0175467  -0.05769275 -0.08599418 -0.09155273  0.0423119\n",
      " -0.14993068  0.07241385 -0.04759707  0.06599499 -0.02213396 -0.01985386\n",
      "  0.03608922  0.0730242   0.04145477 -0.0592204  -0.03368414 -0.14240374\n",
      " -0.11207799  0.04079474 -0.13815162  0.18583752 -0.0264631   0.13613746\n",
      "  0.04344395  0.10146368 -0.1568371  -0.09093766  0.01943534 -0.12862142\n",
      " -0.09729658 -0.08818272 -0.1499038  -0.02244436 -0.04221671 -0.14467076\n",
      " -0.08727664  0.08703323 -0.03724452  0.03713844  0.00841413 -0.03819638\n",
      "  0.04871187  0.00458563 -0.00879088 -0.01597087  0.09444028  0.0128588\n",
      "  0.00474766 -0.07494536 -0.1058989   0.0075887  -0.01565479 -0.01770601\n",
      "  0.09909348  0.05438378 -0.00987607  0.16372826  0.04281544  0.07157389\n",
      "  0.02041335 -0.01892671 -0.03254191 -0.07805089  0.03469703  0.0835237\n",
      " -0.05856469  0.07807995  0.18223935 -0.03905814 -0.06559826  0.00441778\n",
      " -0.04765683  0.00174967 -0.0499442   0.08921596 -0.05202811  0.02542405\n",
      "  0.02564639  0.03856695 -0.08229138 -0.13677761 -0.03282129 -0.00445811\n",
      "  0.02180172  0.11124965  0.06431825 -0.01736886 -0.0168537  -0.02965527\n",
      "  0.09707932  0.06195359 -0.00590007  0.01967657  0.10620698 -0.11439151\n",
      " -0.08102853 -0.02657209  0.03414045  0.09706279 -0.04822213 -0.02752686\n",
      " -0.05930801 -0.04303269 -0.11006728  0.07374627 -0.02335757  0.02132161\n",
      "  0.16190592  0.10704259  0.1453654  -0.04960033  0.01669021 -0.0568092\n",
      " -0.0281808  -0.08471525 -0.04839943  0.12519764 -0.04938035  0.01867639\n",
      "  0.05154128 -0.21320452 -0.04382438 -0.02979388 -0.07316953 -0.07876006\n",
      "  0.0666137   0.11970738 -0.02038502 -0.02043225 -0.00854492  0.06244187\n",
      "  0.00406247 -0.01388695 -0.00756109 -0.00510007  0.11270578 -0.00383032\n",
      " -0.07336771  0.05388387 -0.06600226 -0.08736456 -0.03554935 -0.02983529\n",
      " -0.02234432  0.16597493  0.20271229 -0.15409924  0.02767508 -0.06028275\n",
      "  0.01874506  0.00330793  0.01029487 -0.05950346 -0.03407215  0.05586279\n",
      " -0.05255999  0.06726728 -0.03091867  0.03321257  0.09562029 -0.0280878\n",
      " -0.18580264 -0.00352623  0.09402902  0.04684158 -0.06187584 -0.01583354\n",
      "  0.04801396 -0.06676665 -0.07035755  0.01950655 -0.13340541 -0.09078544\n",
      "  0.05690802 -0.08546084 -0.01820592  0.07843744 -0.03812554  0.11159842\n",
      "  0.11969721  0.0449335  -0.10582152  0.00232224 -0.05729457  0.03292856\n",
      "  0.10434396 -0.05161422 -0.12914458 -0.00114223 -0.09456671  0.08707973\n",
      "  0.03819348 -0.02696519 -0.02721441 -0.08526757  0.04842413  0.01009987\n",
      "  0.01225503 -0.11512538  0.02444313 -0.03519767 -0.05714451  0.00021944\n",
      " -0.00073242  0.00945232  0.02562932  0.08069402  0.05912563  0.06553432\n",
      "  0.04624358  0.05097017  0.21736072 -0.03040495 -0.01796759  0.03606669\n",
      " -0.11412993  0.12206305  0.030553   -0.05254146  0.03547015  0.01890128\n",
      "  0.06088838  0.08702923 -0.02756319 -0.09857505 -0.04788281  0.05559721\n",
      "  0.02226257 -0.06011454  0.04666101  0.00855019  0.0753697  -0.00653367\n",
      " -0.0653454   0.00939142  0.00217983 -0.00272115 -0.01113456 -0.02284095\n",
      " -0.05891854 -0.00576637 -0.01346842  0.02164714  0.04594494 -0.06361171\n",
      " -0.12094988 -0.09827241 -0.05226789  0.04806519 -0.06530035  0.07831682\n",
      " -0.00723557 -0.01486896 -0.00217438  0.04452805 -0.00202724  0.02008347\n",
      " -0.03751337 -0.02039809  0.05640811 -0.01563081 -0.08022272  0.06058466\n",
      " -0.02697173 -0.09694708 -0.0692749  -0.01718285 -0.10185896  0.12101673]\n"
     ]
    }
   ],
   "source": [
    "embeddings_word2vec = get_embeddings(word2vec_model, train_df[\"lemmatize_text\"], k=300)\n",
    "\n",
    "print(\"Embedding matrix size\", len(embeddings_word2vec), len(embeddings_word2vec[0]))\n",
    "print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n",
    "print(embeddings_word2vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GloVe\"></a>\n",
    "\n",
    "#### Global Vectors for Word Representation (GloVe):\n",
    "> Another powerful word embedding technique that has been used for text classiﬁcation is [Global Vectors (GloVe)](https://nlp.stanford.edu/pubs/glove.pdf). The approach is very similar to the Word2Vec method, where each word is presented by a high dimension vector and trained based on the surrounding words over a huge corpus. The pre-trained word embedding used in many works is based on 400,000 vocabularies trained over Wikipedia 2014 and Gigaword 5 as the corpus and 50 dimensions for word presentation. GloVe also provides other pre-trained word vectorizations with 100, 200, 300 dimensions which are trained over even bigger corpora, including Twitter content.\n",
    "\n",
    "(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
    "\n",
    "We will create our GloVe's sentence embeddings  via gensim libary with the pre-trained word vectors on the dataset from Wikipedia 2014 + Gigaword 5 (source: https://github.com/stanfordnlp/GloVe) and see the embedding output on the sample sentence from the our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove_input_file = \"../input/glove6b/glove.6B.300d.txt\"\n",
    "# word2vec_output_file = \"glove.6B.100d.txt.word2vec\"\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, limit=200000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glove_model.similarity('cat', 'kitten'))\n",
    "# print(glove_model.similarity('cat', 'cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_glove = get_embeddings(glove_model, train_df[\"lemmatize_text\"], k=300)\n",
    "\n",
    "# print(\"Embedding matrix size\", len(embeddings_glove), len(embeddings_glove[0]))\n",
    "# print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n",
    "# print(embeddings_glove[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del embeddings_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FastText\"></a>\n",
    "\n",
    "#### FastText:\n",
    "> Many other word embedding representations ignore the morphology of words by assigning a distinct vector to each word ([Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)). Facebook AI Research lab released a novel technique to solve this issue by introducing a new word embedding method called FastText. Each word, w, is represented as a bag of character n-gram. For example, given the word “introduce” and n = 3, FastText will produce the following representation composed of character tri-grams: < in, int, ntr, tro, rod, odu, duc, uce, ce >\n",
    "> Note that the sequence <int>, corresponding to the word here is different from the tri-gram “int” from the word introduce.\n",
    "\n",
    "(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n",
    "\n",
    "We will create our FastText's sentence embeddings via gensim libary with the pre-trained word vectors from the Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (source: https://fasttext.cc/docs/en/english-vectors.html) and see the embedding output on the sample sentence from the our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "fasttext_path = r\"D:\\Sharif University of Tech\\Data\\NLP Feature Extraction\\Data\\fastText English Word Vectors\\wiki-news-300d-1M.vec\"\n",
    "fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_path, binary=False, limit=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7353649\n",
      "0.85528094\n"
     ]
    }
   ],
   "source": [
    "print(fasttext_model.similarity('cat', 'kitten'))\n",
    "print(fasttext_model.similarity('cat', 'cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size 7613 300\n",
      "The sentence: \"deed reason earthquake may allah forgive u\" got embedding values: \n",
      "[-8.09166668e-02  2.61404759e-02 -8.73142836e-02 -3.69428569e-02\n",
      "  1.73714293e-02  4.03738089e-02  1.19119056e-02 -2.96238094e-02\n",
      "  1.21804762e-01  4.34142850e-02 -2.36714286e-02 -2.98738095e-02\n",
      "  5.00333325e-02 -2.49952377e-02 -2.62000006e-02  2.73166672e-02\n",
      " -1.57404756e-02 -1.61595243e-02 -2.85976184e-02  7.77619022e-02\n",
      " -1.63690476e-02 -1.13026191e-01 -3.10547621e-02 -2.75714294e-02\n",
      " -6.89023813e-02 -5.33142867e-02  2.94666670e-02  1.26283331e-01\n",
      "  3.59476194e-02 -9.56166665e-02  4.42857155e-02  2.53380938e-02\n",
      " -1.39238087e-02 -2.98023802e-02  5.93452381e-02  6.58619040e-02\n",
      " -1.11547623e-02  1.53952374e-02  5.92904766e-02 -1.74500004e-02\n",
      " -2.33333374e-03 -9.57619045e-03 -9.73023806e-02 -3.89999918e-03\n",
      "  2.21000003e-02  1.47904770e-02  4.90023823e-02  2.88071422e-02\n",
      "  8.64047606e-02  3.05714297e-03 -1.81666667e-02 -4.32404758e-02\n",
      " -5.74880958e-01 -4.76190484e-03  1.78571507e-03  5.96190463e-03\n",
      " -7.99999836e-04 -9.34619041e-02  5.42380883e-03 -2.31666604e-03\n",
      "  1.85404762e-02  1.76595233e-02 -1.57778573e-01  4.48833325e-02\n",
      " -3.58809517e-02 -2.47214290e-02  4.40095242e-02  2.98309520e-02\n",
      "  3.23642852e-02  5.24619048e-02  1.51547627e-02 -1.32761908e-02\n",
      "  2.10095238e-02  1.50040474e-01  1.93047614e-02 -4.03571388e-03\n",
      " -5.91261903e-02 -1.90238092e-02  3.94095237e-02 -8.10119056e-02\n",
      "  1.22333335e-02 -1.63928571e-02 -6.08333396e-03 -1.18854762e-01\n",
      "  1.85023812e-02  2.22166662e-02 -6.20333342e-02  1.24119048e-02\n",
      "  2.72238107e-02 -4.28428577e-02  4.58404764e-02  2.52214282e-02\n",
      "  4.67428578e-02 -6.18333340e-02 -2.93333303e-03  3.91785722e-02\n",
      "  3.22309522e-02 -2.98333329e-02 -1.15523810e-02 -2.83334146e-04\n",
      " -1.57207144e-01 -9.74761843e-03 -2.93095258e-03 -9.84285681e-03\n",
      " -7.68523802e-02  1.80357150e-02  4.69071407e-02  1.51190480e-02\n",
      " -8.97309527e-02 -2.44333342e-02 -2.22928570e-02  2.20238109e-03\n",
      "  8.87952396e-02  4.39285726e-02 -4.41071433e-02  5.92119054e-02\n",
      "  1.95119035e-02 -7.48785725e-02 -6.03238098e-02 -2.65961904e-01\n",
      "  2.31119053e-02 -1.87547626e-02 -1.92880952e-02 -6.99690467e-02\n",
      " -3.20761907e-02  1.63716669e-01  3.84000000e-02  5.32142836e-03\n",
      " -3.99547629e-02  2.29809527e-02  1.75880954e-02  3.33761912e-02\n",
      "  3.59761898e-02  4.93857131e-02 -5.94404758e-02 -1.39450000e-01\n",
      " -5.92142876e-03 -1.01619044e-02 -4.32904763e-02  2.35785715e-02\n",
      "  3.56904700e-03 -7.57142836e-03  5.80000009e-02  2.36421428e-01\n",
      "  3.52666676e-02 -4.38666667e-02  5.65428577e-02  7.51428558e-02\n",
      "  3.42142869e-02  1.10952378e-03  1.58357141e-02  5.00761900e-02\n",
      " -3.63261901e-02  3.60952380e-03 -2.46190482e-02  4.78738091e-02\n",
      "  2.04285703e-03  8.73333297e-03 -4.32666671e-02 -7.10714309e-03\n",
      "  1.35309529e-02 -2.25761906e-02  4.16714291e-02  1.71207142e-01\n",
      " -3.28928582e-02  3.25761900e-02  1.38064284e-01 -2.80333335e-02\n",
      " -1.68095237e-02 -7.23571420e-03  4.00880949e-02 -5.26904726e-03\n",
      " -2.37833326e-02  1.47666670e-02  2.31119058e-02  2.10738094e-02\n",
      "  1.86942857e-01 -7.15714183e-03 -9.38095314e-03 -8.60476179e-03\n",
      "  2.15619037e-02  5.68261901e-02 -2.52523807e-02 -3.76190444e-03\n",
      " -2.91238090e-02 -5.85642854e-02 -3.53404757e-02 -2.57857142e-02\n",
      "  1.39666669e-02  6.91190469e-02  6.11357138e-02  8.65404758e-02\n",
      " -2.64666667e-02  5.13333308e-03 -3.12690476e-02 -4.03880952e-02\n",
      " -3.74380951e-02 -4.73595234e-02  6.42309520e-02 -1.14999986e-03\n",
      "  2.95738095e-02 -9.12857142e-02 -1.66500011e-02 -7.64523819e-03\n",
      "  2.99999973e-04  2.78285714e-02  1.77476186e-02  2.48404759e-02\n",
      "  2.88595242e-02 -3.41119046e-02 -1.61738098e-02 -1.27380963e-02\n",
      " -5.64999943e-03  3.40476186e-02  3.37619047e-02 -6.19428575e-02\n",
      "  1.17735713e-01 -2.08571436e-02 -8.08547635e-02  1.11997618e-01\n",
      " -7.10595230e-02 -3.78833323e-02 -5.70000003e-02 -5.37380936e-02\n",
      " -4.76547623e-02 -1.11830953e-01 -5.19428577e-02  1.74285725e-03\n",
      "  7.24047613e-03 -9.04833336e-02  2.97857144e-02 -1.62190479e-02\n",
      "  2.83530953e-01 -1.18140477e-01  6.94047685e-03 -3.50571431e-02\n",
      " -1.33190472e-02 -9.52452367e-02 -1.73821428e-01 -5.55880951e-02\n",
      "  5.33690458e-02 -2.13309512e-02  2.95000032e-03 -9.39047636e-03\n",
      " -5.94238100e-02  1.11971428e-01  5.07119043e-02 -3.16166669e-02\n",
      "  6.64404747e-02  2.75476190e-01  3.32285713e-02 -3.63523814e-02\n",
      "  1.28280952e-01  3.08809490e-03  3.50428562e-02 -7.12857095e-03\n",
      " -2.11357134e-02 -2.14785713e-02 -2.46714284e-02  4.19523779e-03\n",
      " -5.78095226e-03 -1.01476185e-02  4.82142829e-03  3.13476199e-02\n",
      " -2.66540474e-01  5.98571437e-02  2.34404770e-02 -1.99095237e-02\n",
      "  1.23619061e-02 -2.66809530e-02 -3.52690477e-02  2.13214292e-02\n",
      "  2.06976189e-02  4.77285711e-02 -2.65857133e-02  4.95714291e-03\n",
      " -7.58190487e-02 -8.61452384e-02  3.80499995e-02  3.11595234e-02\n",
      "  2.68809508e-03  5.83547609e-02  2.36285714e-02 -2.55952385e-02\n",
      "  1.26690477e-02 -3.88809529e-02  1.18414287e-01 -1.03952383e-02\n",
      "  9.16666659e-03  4.35738095e-02  2.62380946e-02  1.23714284e-02\n",
      " -4.50238077e-03  4.60690471e-02  2.82595239e-02  2.25785720e-02\n",
      "  1.77857139e-02 -2.51190479e-02  2.50714419e-03  6.78095219e-03]\n"
     ]
    }
   ],
   "source": [
    "embeddings_fasttext = get_embeddings(fasttext_model, train_df[\"lemmatize_text\"], k=300)\n",
    "\n",
    "print(\"Embedding matrix size\", len(embeddings_fasttext), len(embeddings_fasttext[0]))\n",
    "print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n",
    "print(embeddings_fasttext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Advanced_methods\"></a>\n",
    "\n",
    "### Advanced Word Embedding Methods - Deep Contextualized Word Representations: \n",
    "\n",
    "<a id=\"BERT\"></a>\n",
    "#### Bidirectional Encoder Representations from Transformers (BERT):\n",
    "> BERT is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pre-trained on Wikipedia and BooksCorpus and requires task-specific fine-tuning.\n",
    "\n",
    "> Lets understand BERT by breaking BERT abbreviation:\n",
    "> * **Bidirectional**: BERT takes whole text passage as input and reads passage in both direction to understand the meaning of each word.\n",
    "> * **Transformers**: BERT is based on a Deep Transformer network. Transformer network is a type of network that can process efficiently long texts by using attention. An attention is a mechanism to learn contextual relations between words (or sub-words) in a text.\n",
    "> * **Encoder Representation**: Originally Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task, since BERT’s goal is to generate a language model only the encoder mechanism is necessary hence 'encoder representation'\n",
    "\n",
    "> BERT is a multi-layer bidirectional Transformer encoder. There are two models introduced in the paper.\n",
    "> * BERT base – 12 layers (transformer blocks), 12 attention heads, and 110 million parameters.\n",
    "> * BERT Large – 24 layers, 16 attention heads and, 340 million parameters.\n",
    "\n",
    "\n",
    "> How BERT performs Bidirectional training?\n",
    "> \n",
    "> BERT uses following two prediction models simultaneously with the goal of minimizing the combined loss function of the two strategies:\n",
    "> \n",
    "> * **Masked Language Model**: Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n",
    "> * **Next Sentence Prediction**: The model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n",
    "\n",
    "Resources and further reading on BERT's explanation could be found in the great Kaggle notebooks and Blogs here:\n",
    "* https://www.kaggle.com/abhinand05/bert-for-humans-tutorial-baseline-version-2\n",
    "* https://www.kaggle.com/ratan123/in-depth-guide-to-google-s-bert\n",
    "* https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-3-bert\n",
    "* https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/\n",
    "\n",
    "We will create our sentence embeddings by BERT's pre-trained word vectors (Uncased) via Tensorflow (source: https://github.com/google-research/bert) and see the embedding output on the sample sentence from the our dataset. Noted that we will use the BERT isself tonkenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\"\n",
    "r = requests.get(url)\n",
    "with open('tokenization.py', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (tokenization.py, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3526\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[94], line 1\u001b[1;36m\n\u001b[1;33m    import tokenization\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32md:\\Sharif University of Tech\\Data\\NLP Feature Extraction\\Code\\tokenization.py:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    404: Not Found\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "import tokenization\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "bert_input = bert_encode(train_df[\"text\"].values, tokenizer, max_len=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding tensor size\", len(bert_input), len(bert_input[0]), len(bert_input[0][0]))\n",
    "print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n",
    "print(bert_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Comparison\"></a>\n",
    "## Comparison of Feature Extraction Techniques\n",
    "Please refer to the below table as the Comparison between Feature Extraction Techniques, thanks to the paper [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067) for all of their awesome works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                               \t| Advantages                                                                                                                                                                                                                                                                             \t| Limitation                                                                                                                                                                                                                                                                                                                                                                  \t|\n",
    "|-------------------------------------\t|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| Weighted Words                      \t| * Easy to compute<br>* Easy to compute the similarity between 2 documents using it<br>* Basic metric to extract the most descriptive terms in a document<br>* Works with an unknown word (e.g., New words in languages)                                                                \t| * It does not capture the position in the text (syntactic)<br>* It does not capture meaning in the text (semantics)<br>* Common words effect on the results (e.g., “am”, “is”, etc.)                                                                                                                                                                                        \t|\n",
    "| TF-IDF                              \t| * Easy to compute<br>* Easy to compute the similarity between 2 documents using it<br>* Basic metric to extract the most descriptive terms in a document<br>* Common words do not affect the results due to IDF (e.g., “am”, “is”, etc.)                                               \t| * It does not capture the position in the text (syntactic)<br>* It does not capture meaning in the text (semantics)                                                                                                                                                                                                                                                         \t|\n",
    "| Word2Vec                            \t| * It captures the position of the words in the text (syntactic)<br>* It captures meaning in the words (semantics)                                                                                                                                                                      \t| * It cannot capture the meaning of the word from the text (fails to capture polysemy)<br>* It cannot capture out-of-vocabulary words from corpus                                                                                                                                                                                                                            \t|\n",
    "| GloVe (Pre-Trained)                 \t| * It captures the position of the words in the text (syntactic)<br>* It captures meaning in the words (semantics)<br>* Trained on huge corpus                                                                                                                                          \t| * It cannot capture the meaning of the word from the text (fails to capture polysemy)<br>* Memory consumption for storage<br>* It cannot capture out-of-vocabulary words from corpus                                                                                                                                                                                        \t|\n",
    "| GloVe (Trained)                     \t| * It is very straightforward, e.g., to enforce the word vectors to capture sub-linear relationships in the vector space (performs better than Word2vec)<br>* Lower weight for highly frequent word pairs, such as stop words like “am”, “is”, etc. Will not dominate training progress \t| * Memory consumption for storage<br>* Needs huge corpus to learn<br>* It cannot capture out-of-vocabulary words from the corpus<br>* It cannot capture the meaning of the word from the text (fails to capture polysemy)                                                                                                                                                    \t|\n",
    "| FastText                            \t| * Works for rare words (rare in their character n-grams which are still shared with other words<br>* Solves out of vocabulary words with n-gram in character level                                                                                                                     \t| * It cannot capture the meaning of the word from the text (fails to capture polysemy)<br>* Memory consumption for storage<br>* Computationally is more expensive in comparing with GloVe and Word2Vec                                                                                                                                                                       \t|\n",
    "| Contextualized Word Representations \t| * It captures the meaning of the word from the text (incorporates context, handling polysemy)                                                                                                                                                                                          \t| * Memory consumption for storage<br>* Improves performance notably on downstream tasks. Computationally is more expensive in comparison to others<br>* Needs another word embedding for all LSTM and feedforward layers<br>* It cannot capture out-of-vocabulary words from a corpus<br>* Works only sentence and document level (it cannot work for individual word level) \t|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
